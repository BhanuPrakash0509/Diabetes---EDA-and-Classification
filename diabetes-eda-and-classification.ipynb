{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"border-radius: 10px; background-color: #A478B8\">\n    <h1 style=\"color: white; padding: 1rem\">Introduction</h1>\n</div>\n\nThis is a beginner-friendly notebook that attempts to perform **Exploratory Data Analysis** on the **[Pima Indians Diabetes Dataset](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database)** and eventually train a Machine Learning model on it and enhance the predictions by fine-tuning the model.","metadata":{}},{"cell_type":"code","source":"# for data wrangling\nimport numpy as np\nimport pandas as pd\n\n# for data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# modelling\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, VotingClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import SGDClassifier\n\nfrom sklearn.model_selection import cross_val_predict, RandomizedSearchCV, train_test_split\nfrom sklearn.metrics import classification_report, PrecisionRecallDisplay, RocCurveDisplay, accuracy_score, confusion_matrix, ConfusionMatrixDisplay","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:46.421632Z","iopub.execute_input":"2022-11-05T11:44:46.422176Z","iopub.status.idle":"2022-11-05T11:44:46.431608Z","shell.execute_reply.started":"2022-11-05T11:44:46.422135Z","shell.execute_reply":"2022-11-05T11:44:46.430313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-11-05T11:44:46.45819Z","iopub.execute_input":"2022-11-05T11:44:46.458755Z","iopub.status.idle":"2022-11-05T11:44:46.468623Z","shell.execute_reply.started":"2022-11-05T11:44:46.45871Z","shell.execute_reply":"2022-11-05T11:44:46.467096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius: 10px; background-color: #A478B8\">\n    <h1 style=\"color: white; padding: 1rem\">Know the Data</h1>\n</div>\n\nLet us first look at what information the dataset contains. One thing to note is, all patients here are females **at least 21 years old** of Pima Indian heritage.\n\nThe dataset has the following **features** (columns):\n\n- **<span style=\"color: darkblue\">Pregnancies</span>**: Number of times pregnant\n- **<span style=\"color: darkblue\">Glucose</span>**: Plasma glucose concentration, 2 hours in an oral glucose tolerance test\n- **<span style=\"color: darkblue\">BloodPressure</span>**: Diastolic blood pressure ($mm \\cdot Hg$)\n- **<span style=\"color: darkblue\">SkinThickness</span>**: Triceps skin fold thickness ($mm$)\n- **<span style=\"color: darkblue\">Insulin</span>**: 2-Hour serum insulin ($mu \\cdot \\dfrac{U} {ml}$)\n- **<span style=\"color: darkblue\">BMI</span>**: Body mass index (weight in $kg$ / height in $m^2$)\n- **<span style=\"color: darkblue\">DiabetesPedigreeFunction</span>**: Diabetes pedigree function (indicates the function which scores likelihood of diabetes based on family history)\n- **<span style=\"color: darkblue\">Age</span>**: Age (in years)\n- **<span style=\"color: darkblue\">Outcome</span>**: Whether patient is diagnosed with Diabetes (0 for No, 1 for Yes)\n\n**<span style=\"color: purple\"> Outcome is our target variable </span>**.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius: 10px; background-color: #A478B8\">\n    <h1 style=\"color: white; padding: 1rem\">Load the dataset</h1>\n</div>\n\nUsing **Pandas** we load the dataset.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\")\ndf","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:46.480464Z","iopub.execute_input":"2022-11-05T11:44:46.48179Z","iopub.status.idle":"2022-11-05T11:44:46.510974Z","shell.execute_reply.started":"2022-11-05T11:44:46.481728Z","shell.execute_reply":"2022-11-05T11:44:46.509753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"An important thing to note here is that:\n\n> This Pima Indians Diabetes dataset contains only **768 rows** and **9 features**. If we want to split the dataset into training and testing set, we need to ensure that the test set is *representative* of the whole dataset. This helps in avoiding the **Sampling Bias** when the training and testing sets are chosen randomly.","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:46.513664Z","iopub.execute_input":"2022-11-05T11:44:46.514068Z","iopub.status.idle":"2022-11-05T11:44:46.531392Z","shell.execute_reply.started":"2022-11-05T11:44:46.51399Z","shell.execute_reply":"2022-11-05T11:44:46.529841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We clearly see that apart from the feature `Outcome`, every other feature is numerical and continuous in nature.","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:46.549434Z","iopub.execute_input":"2022-11-05T11:44:46.549956Z","iopub.status.idle":"2022-11-05T11:44:46.599871Z","shell.execute_reply.started":"2022-11-05T11:44:46.549914Z","shell.execute_reply":"2022-11-05T11:44:46.598411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**<span style=\"color: purple\">Note: The minimum value of the features `Glucose`, `BloodPressure`, `SkinThickness`, `Insulin` and `BMI` is 0. This is logically incorrect as these values cannot be 0. Thus, this can be safely called \"missing data\" in our case. We need to either drop the 0-valued rows or we need to replace them with the *mean* or *median* value of that feature.</span>**\n\nLet's check for null values (if any):","metadata":{}},{"cell_type":"code","source":"# helper function\ndef count_na(df, col):\n    print(f\"Null values in {col}: \", df[col].isna().sum())\n\nfor feat in df.columns:\n    count_na(df, feat)","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:46.615321Z","iopub.execute_input":"2022-11-05T11:44:46.616396Z","iopub.status.idle":"2022-11-05T11:44:46.627665Z","shell.execute_reply.started":"2022-11-05T11:44:46.616339Z","shell.execute_reply":"2022-11-05T11:44:46.62606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Good! There are no null values in our dataset. Let us move on to *visualizing* the dataset to gather more insights about the data.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius: 10px; background-color: #A478B8\">\n    <h1 style=\"color: white; padding: 1rem\">Data visualization</h1>\n</div>\n\nIn this section, we will start visualizing the features of the dataset one by one. Firstly, **Univariate** feature visualization will be done, then we will move onto **Multivariate** feature visualization.\n\n> To learn more about what **graphs** are useful for what **data-types**, check out this notebook here: [Statistical Data Types and Graphs (using Seaborn)](https://www.kaggle.com/code/maharshipandya/statistical-data-types-and-graphs-using-seaborn)","metadata":{}},{"cell_type":"code","source":"# Setting some styles\nsns.set_style(\"darkgrid\")\nsns.set_palette(\"viridis\")","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-11-05T11:44:46.641643Z","iopub.execute_input":"2022-11-05T11:44:46.64337Z","iopub.status.idle":"2022-11-05T11:44:46.651715Z","shell.execute_reply.started":"2022-11-05T11:44:46.643296Z","shell.execute_reply":"2022-11-05T11:44:46.650118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"color: darkblue\">Univariate Analysis</h1>\n<hr/>","metadata":{}},{"cell_type":"markdown","source":"### Analysis of Pregnancies\n\nAs observed, `Pregnancies` is a **Quantitative** feature. There are many plots to analyse these type of data. Histograms, Box plots and Violin plots, are useful to know how the data is distributed.","metadata":{}},{"cell_type":"code","source":"fig1, ax1 = plt.subplots(1, 2, figsize=(20, 7))\nfig2, ax2 = plt.subplots(figsize=(20, 7))\n\nsns.histplot(data=df, x=\"Pregnancies\", kde=True, ax=ax1[0])\nsns.boxplot(data=df, x=\"Pregnancies\", ax=ax1[1])\n\nsns.violinplot(data=df, x=\"Pregnancies\", ax=ax2)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:46.671348Z","iopub.execute_input":"2022-11-05T11:44:46.671947Z","iopub.status.idle":"2022-11-05T11:44:47.410061Z","shell.execute_reply.started":"2022-11-05T11:44:46.671834Z","shell.execute_reply":"2022-11-05T11:44:47.409045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Median of Pregnancies: \", df[\"Pregnancies\"].median())\nprint(\"Maximum of Pregnancies: \", df[\"Pregnancies\"].max())","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:47.411923Z","iopub.execute_input":"2022-11-05T11:44:47.41308Z","iopub.status.idle":"2022-11-05T11:44:47.420287Z","shell.execute_reply.started":"2022-11-05T11:44:47.41304Z","shell.execute_reply":"2022-11-05T11:44:47.419049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Pregnancies\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:47.421847Z","iopub.execute_input":"2022-11-05T11:44:47.422359Z","iopub.status.idle":"2022-11-05T11:44:47.441188Z","shell.execute_reply.started":"2022-11-05T11:44:47.422314Z","shell.execute_reply":"2022-11-05T11:44:47.440064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above analysis we observe that:\n\n- Most patients had 0, 1 or 2 pregnancies.\n- Median value of `Pregnancies` is **3**.\n- Also, patients had upto **17** pregnancies!\n\nThere are 3 outliers on the boxplot. But, let's not remove them for now.","metadata":{}},{"cell_type":"markdown","source":"### Analysis of Outcome (Target Variable)\n\nA Count plot and a Pie chart will be two useful plots to analyse the `Outcome` column as it is a categorical feature. Usefulness in the sense, both the plots will allow us to observe the distribution of each category in the feature.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(20, 7))\n\nsns.countplot(data=df, x=\"Outcome\", ax=ax[0])\ndf[\"Outcome\"].value_counts().plot.pie(explode=[0.1, 0], autopct=\"%1.1f%%\", labels=[\"No\", \"Yes\"], shadow=True, ax=ax[1])\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:47.444294Z","iopub.execute_input":"2022-11-05T11:44:47.444909Z","iopub.status.idle":"2022-11-05T11:44:47.756735Z","shell.execute_reply.started":"2022-11-05T11:44:47.444874Z","shell.execute_reply":"2022-11-05T11:44:47.75552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe from the above plot that:\n\n- **65.1% patients in the dataset do NOT have diabetes.**\n- **34.9% patients in the dataset has diabetes.**","metadata":{}},{"cell_type":"markdown","source":"### Analysis of Glucose\n\n`Glucose` is a **Quantitative** feature. Histograms, Box plots and Violin plots, are useful to know how the data is distributed.","metadata":{}},{"cell_type":"code","source":"fig3, ax3 = plt.subplots(1, 2, figsize=(20, 7))\nfig4, ax4 = plt.subplots(figsize=(20, 7))\n\nsns.histplot(data=df, x=\"Glucose\", kde=True, ax=ax3[0])\nsns.boxplot(data=df, x=\"Glucose\", ax=ax3[1])\n\nsns.violinplot(data=df, x=\"Glucose\", ax=ax4)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:47.75825Z","iopub.execute_input":"2022-11-05T11:44:47.758691Z","iopub.status.idle":"2022-11-05T11:44:48.497589Z","shell.execute_reply.started":"2022-11-05T11:44:47.758659Z","shell.execute_reply":"2022-11-05T11:44:48.496724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Median of Glucose: \", df[\"Glucose\"].median())\nprint(\"Maximum of Glucose: \", df[\"Glucose\"].max())\nprint(\"Mean of Glucose: \", df[\"Glucose\"].mean())","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:48.498805Z","iopub.execute_input":"2022-11-05T11:44:48.499479Z","iopub.status.idle":"2022-11-05T11:44:48.50755Z","shell.execute_reply.started":"2022-11-05T11:44:48.499418Z","shell.execute_reply":"2022-11-05T11:44:48.506233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Rows with Glucose value of 0: \", df[df[\"Glucose\"] == 0].shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:48.509029Z","iopub.execute_input":"2022-11-05T11:44:48.50943Z","iopub.status.idle":"2022-11-05T11:44:48.522541Z","shell.execute_reply.started":"2022-11-05T11:44:48.509394Z","shell.execute_reply":"2022-11-05T11:44:48.521107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that:\n\n- Median (117.0) and mean (120.8) of `Glucose` lie very close to each other i.e. the distribution is more or less **symmetric and uniform**.\n- As seen from the box plot, an outlier lies on 0-value, which I talked about earlier.\n- There are **5 rows** with `Glucose` value as 0. This is not logical, so we need to keep this in mind.","metadata":{}},{"cell_type":"markdown","source":"### Analysis of Blood Pressure\n\n`BloodPressure` is a **Quantitative** feature. Histograms, Box plots and Violin plots, are useful to know how the data is distributed.","metadata":{}},{"cell_type":"code","source":"fig5, ax5 = plt.subplots(1, 2, figsize=(20, 7))\nfig6, ax6 = plt.subplots(figsize=(20, 7))\n\nsns.histplot(data=df, x=\"BloodPressure\", kde=True, ax=ax5[0])\nsns.boxplot(data=df, x=\"BloodPressure\", ax=ax5[1])\n\nsns.violinplot(data=df, x=\"BloodPressure\", ax=ax6)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:48.524738Z","iopub.execute_input":"2022-11-05T11:44:48.525157Z","iopub.status.idle":"2022-11-05T11:44:49.265278Z","shell.execute_reply.started":"2022-11-05T11:44:48.525123Z","shell.execute_reply":"2022-11-05T11:44:49.264029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Median of Blood Pressure: \", df[\"BloodPressure\"].median())\nprint(\"Maximum of Blood Pressure: \", df[\"BloodPressure\"].max())\nprint(\"Mean of Pressure: \", df[\"BloodPressure\"].mean())","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:49.266912Z","iopub.execute_input":"2022-11-05T11:44:49.267292Z","iopub.status.idle":"2022-11-05T11:44:49.274169Z","shell.execute_reply.started":"2022-11-05T11:44:49.267259Z","shell.execute_reply":"2022-11-05T11:44:49.273008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Rows with BloodPressure value of 0: \", df[df[\"BloodPressure\"] == 0].shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:49.278402Z","iopub.execute_input":"2022-11-05T11:44:49.278804Z","iopub.status.idle":"2022-11-05T11:44:49.289126Z","shell.execute_reply.started":"2022-11-05T11:44:49.278769Z","shell.execute_reply":"2022-11-05T11:44:49.287882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that:\n​\n- Median (72.0) and mean (69.1) of `BloodPressure` lie very close to each other i.e. the distribution is more or less **symmetric and uniform**.\n- As seen from the box plot and violin plot, some outliers lie on 0-value, which I talked about earlier.\n- There are **35 rows** with `BloodPressure` value as 0. This is not logical.","metadata":{}},{"cell_type":"markdown","source":"### Analysis of Insulin\n\nPlotting Histogram, Box plot and Violin plot for `Insulin`.","metadata":{}},{"cell_type":"code","source":"fig7, ax7 = plt.subplots(1, 2, figsize=(20, 7))\nfig8, ax8 = plt.subplots(figsize=(20, 7))\n\nsns.histplot(data=df, x=\"Insulin\", kde=True, ax=ax7[0])\nsns.boxplot(data=df, x=\"Insulin\", ax=ax7[1])\n\nsns.violinplot(data=df, x=\"Insulin\", ax=ax8)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:49.290654Z","iopub.execute_input":"2022-11-05T11:44:49.291058Z","iopub.status.idle":"2022-11-05T11:44:49.997355Z","shell.execute_reply.started":"2022-11-05T11:44:49.29099Z","shell.execute_reply":"2022-11-05T11:44:49.995949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Rows with Insulin value of 0: \", df[df[\"Insulin\"] == 0].shape[0])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-11-05T11:44:49.99914Z","iopub.execute_input":"2022-11-05T11:44:49.999985Z","iopub.status.idle":"2022-11-05T11:44:50.007172Z","shell.execute_reply.started":"2022-11-05T11:44:49.999937Z","shell.execute_reply":"2022-11-05T11:44:50.006188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The plots for `Insulin` are highly skewed. Also, the 0-value logical error is the most for this feature. **374 out of 768** instances have value of `Insulin` as 0.","metadata":{}},{"cell_type":"markdown","source":"### Analysis of BMI\n\nPlotting Histogram, Box plot and Violin plot for `BMI`.","metadata":{}},{"cell_type":"code","source":"fig9, ax9 = plt.subplots(1, 2, figsize=(20, 7))\nfig10, ax10 = plt.subplots(figsize=(20, 7))\n\nsns.histplot(data=df, x=\"BMI\", kde=True, ax=ax9[0])\nsns.boxplot(data=df, x=\"BMI\", ax=ax9[1])\n\nsns.violinplot(data=df, x=\"BMI\", ax=ax10)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:50.008317Z","iopub.execute_input":"2022-11-05T11:44:50.0094Z","iopub.status.idle":"2022-11-05T11:44:50.780734Z","shell.execute_reply.started":"2022-11-05T11:44:50.00936Z","shell.execute_reply":"2022-11-05T11:44:50.779497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Median of BMI: \", df[\"BMI\"].median())\nprint(\"Maximum of BMI: \", df[\"BMI\"].max())\nprint(\"Mean of BMI: \", df[\"BMI\"].mean())","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:50.781961Z","iopub.execute_input":"2022-11-05T11:44:50.782295Z","iopub.status.idle":"2022-11-05T11:44:50.791174Z","shell.execute_reply.started":"2022-11-05T11:44:50.782266Z","shell.execute_reply":"2022-11-05T11:44:50.789973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Rows with BMI value of 0: \", df[df[\"BMI\"] == 0].shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:50.792914Z","iopub.execute_input":"2022-11-05T11:44:50.793388Z","iopub.status.idle":"2022-11-05T11:44:50.808289Z","shell.execute_reply.started":"2022-11-05T11:44:50.793345Z","shell.execute_reply":"2022-11-05T11:44:50.806734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that:\n\n- Median (32.0) and Mean (31.9) of `BMI` are very close to each other. Thus, the distribution is more or less **symmetric and uniform**\n- Maximum BMI is 67.1\n- There are **11 rows** with `BMI` value as 0","metadata":{}},{"cell_type":"markdown","source":"### Analysis of Diabetes Pedigree Function\n\n`DiabetesPedigreeFunction` is a **continuous and quantitative** variable.","metadata":{}},{"cell_type":"code","source":"fig11, ax11 = plt.subplots(1, 2, figsize=(20, 7))\nfig12, ax12 = plt.subplots(figsize=(20, 7))\n\nsns.histplot(data=df, x=\"DiabetesPedigreeFunction\", kde=True, ax=ax11[0])\nsns.boxplot(data=df, x=\"DiabetesPedigreeFunction\", ax=ax11[1])\n\nsns.violinplot(data=df, x=\"DiabetesPedigreeFunction\", ax=ax12)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:50.810119Z","iopub.execute_input":"2022-11-05T11:44:50.810647Z","iopub.status.idle":"2022-11-05T11:44:51.534164Z","shell.execute_reply.started":"2022-11-05T11:44:50.810599Z","shell.execute_reply":"2022-11-05T11:44:51.532941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Median of DiabetesPedigreeFunction: \", df[\"DiabetesPedigreeFunction\"].median())\nprint(\"Maximum of DiabetesPedigreeFunction: \", df[\"DiabetesPedigreeFunction\"].max())\nprint(\"Mean of DiabetesPedigreeFunction: \", df[\"DiabetesPedigreeFunction\"].mean())","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:51.535839Z","iopub.execute_input":"2022-11-05T11:44:51.536288Z","iopub.status.idle":"2022-11-05T11:44:51.544152Z","shell.execute_reply.started":"2022-11-05T11:44:51.536257Z","shell.execute_reply":"2022-11-05T11:44:51.54287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that:\n\n- The histogram is higly skewed on the left side.\n- There are many outliers in the Box plot.\n- Violin plot distribution is dense in the interval `0.0 - 1.0`","metadata":{}},{"cell_type":"markdown","source":"### Analysis of Age\n\nPlotting Histogram, Box plot and Violin plots for `Age`.","metadata":{}},{"cell_type":"code","source":"fig13, ax13 = plt.subplots(1, 2, figsize=(20, 7))\nfig14, ax14 = plt.subplots(figsize=(20, 7))\n\nsns.histplot(data=df, x=\"Age\", kde=True, ax=ax13[0])\nsns.boxplot(data=df, x=\"Age\", ax=ax13[1])\n\nsns.violinplot(data=df, x=\"Age\", ax=ax14)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:51.545437Z","iopub.execute_input":"2022-11-05T11:44:51.545772Z","iopub.status.idle":"2022-11-05T11:44:53.111802Z","shell.execute_reply.started":"2022-11-05T11:44:51.545741Z","shell.execute_reply":"2022-11-05T11:44:53.110923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Median of Age: \", df[\"Age\"].median())\nprint(\"Maximum of Age: \", df[\"Age\"].max())\nprint(\"Mean of Age: \", df[\"Age\"].mean())","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:53.11338Z","iopub.execute_input":"2022-11-05T11:44:53.113961Z","iopub.status.idle":"2022-11-05T11:44:53.121152Z","shell.execute_reply.started":"2022-11-05T11:44:53.113926Z","shell.execute_reply":"2022-11-05T11:44:53.119841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We again observe that:\n\n- The distribution of Age is skewed on the left side.\n- There are some outliers in the Box plot for Age.","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"color: darkblue\">Multivariate Analysis</h1>\n<hr/>\n\n","metadata":{}},{"cell_type":"markdown","source":"### Analysis of Glucose and Outcome\n\nSince `Glucose` is a continuous feature, we plot a histogram with its hue based on `Outcome`.","metadata":{}},{"cell_type":"code","source":"fig15, ax15 = plt.subplots(figsize=(20, 8))\n\nsns.histplot(data=df, x=\"Glucose\", hue=\"Outcome\", shrink=0.8, multiple=\"fill\", kde=True, ax=ax15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:53.123091Z","iopub.execute_input":"2022-11-05T11:44:53.123785Z","iopub.status.idle":"2022-11-05T11:44:53.632153Z","shell.execute_reply.started":"2022-11-05T11:44:53.123737Z","shell.execute_reply":"2022-11-05T11:44:53.630987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above plot, we see a **positive linear correlation**.\n\n- As the value of `Glucose` increases, the count of patients having diabetes increases i.e. value of `Outcome` as 1, increases.\n- Also, after the `Glucose` value of **125**, there is a steady increase in the number of patients having `Outcome` of 1.\n- Note, when `Glucose` value is 0, it means the measurement is missing. We need to fill that values with the *mean* or *median* and then it will make sense.\n\nSo, there is a significant amount of *positive* linear correlation.","metadata":{}},{"cell_type":"markdown","source":"### Analysis of BloodPressure and Outcome\n\n`BloodPressure` is continuous and `Outcome` is binary feature. So, plotting a histogram for `BloodPressure` with its hue based on `Outcome`.","metadata":{}},{"cell_type":"code","source":"fig16, ax16 = plt.subplots(figsize=(20, 8))\n\nsns.histplot(data=df, x=\"BloodPressure\", hue=\"Outcome\", shrink=0.8, multiple=\"dodge\", kde=True, ax=ax16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:53.634145Z","iopub.execute_input":"2022-11-05T11:44:53.63461Z","iopub.status.idle":"2022-11-05T11:44:54.140934Z","shell.execute_reply.started":"2022-11-05T11:44:53.634566Z","shell.execute_reply":"2022-11-05T11:44:54.13971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that, `Outcome` and `BloodPressure` do **NOT** have a positive or negative linear correlation. The value of `Outcome` do not increase linearly as value of `BloodPressure` increases.\n\nHowever, for `BloodPressure` values greater than 82, count of patients with `Outcome` as 1, is more.","metadata":{}},{"cell_type":"markdown","source":"### Analysis of BMI and Outcome","metadata":{}},{"cell_type":"code","source":"fig17, ax17 = plt.subplots(figsize=(20, 8))\n\nsns.histplot(data=df, x=\"BMI\", hue=\"Outcome\", shrink=0.8, multiple=\"fill\", kde=True, ax=ax17)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:54.142531Z","iopub.execute_input":"2022-11-05T11:44:54.144094Z","iopub.status.idle":"2022-11-05T11:44:54.690333Z","shell.execute_reply.started":"2022-11-05T11:44:54.144044Z","shell.execute_reply":"2022-11-05T11:44:54.689065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above plot, a **positive linear correlation** is evident for `BMI`.","metadata":{}},{"cell_type":"markdown","source":"### Analysis of Age and Outcome\n\n`Age` is continuous so plotting a histogram with hue based on `Outcome`.","metadata":{}},{"cell_type":"code","source":"fig18, ax18 = plt.subplots(figsize=(20, 8))\n\nsns.histplot(data=df, x=\"Age\", hue=\"Outcome\", shrink=0.8, multiple=\"dodge\", kde=True, ax=ax18)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:54.692031Z","iopub.execute_input":"2022-11-05T11:44:54.692494Z","iopub.status.idle":"2022-11-05T11:44:55.146172Z","shell.execute_reply.started":"2022-11-05T11:44:54.692451Z","shell.execute_reply":"2022-11-05T11:44:55.144559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For `Age` greater than 35 years, the chances of patients having diabetes increases as evident from the plot i.e. The number of patients having diabetes is more than the number of people **NOT** having diabetes. But, it does not hold true for ages like **60+**, somehow.\n\nThere is *some* positive linear correlation though.","metadata":{}},{"cell_type":"markdown","source":"### Analysis of Pregnancies and Outcome","metadata":{}},{"cell_type":"code","source":"fig19, ax19 = plt.subplots(figsize=(20, 8))\n\nsns.histplot(data=df, x=\"Pregnancies\", hue=\"Outcome\", shrink=0.8, multiple=\"fill\", kde=True, ax=ax19)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:55.147883Z","iopub.execute_input":"2022-11-05T11:44:55.148504Z","iopub.status.idle":"2022-11-05T11:44:55.59276Z","shell.execute_reply.started":"2022-11-05T11:44:55.148461Z","shell.execute_reply":"2022-11-05T11:44:55.591452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is *some* positive linear correlation of `Pregnancies` with `Outcome`.","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"color: darkblue\">Analyzing Correlations</h1>\n<hr/>\n\nLet us plot a **heatmap** of the correlation matrix of different features.","metadata":{}},{"cell_type":"code","source":"# The 2D correlation matrix\ncorr_matrix = df.corr()","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:55.594226Z","iopub.execute_input":"2022-11-05T11:44:55.594558Z","iopub.status.idle":"2022-11-05T11:44:55.601637Z","shell.execute_reply.started":"2022-11-05T11:44:55.594529Z","shell.execute_reply":"2022-11-05T11:44:55.600247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the heatmap of corr\n\nfig20, ax20 = plt.subplots(figsize=(20, 7))\ndataplot = sns.heatmap(data=corr_matrix, annot=True, ax=ax20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:55.603426Z","iopub.execute_input":"2022-11-05T11:44:55.603768Z","iopub.status.idle":"2022-11-05T11:44:56.260346Z","shell.execute_reply.started":"2022-11-05T11:44:55.603738Z","shell.execute_reply":"2022-11-05T11:44:56.259018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_matrix[\"Outcome\"].sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:56.262112Z","iopub.execute_input":"2022-11-05T11:44:56.263338Z","iopub.status.idle":"2022-11-05T11:44:56.273424Z","shell.execute_reply.started":"2022-11-05T11:44:56.263286Z","shell.execute_reply":"2022-11-05T11:44:56.272267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that:\n\n- `Glucose` has the maximum positive linear correlation with `Outcome`, which is logical.\n- `BloodPressure` has the lowest positive linear correlation with `Outcome`.\n- No feature has a negative linear correlation with `Outcome`.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius: 10px; background-color: #A478B8\">\n    <h1 style=\"color: white; padding: 1rem\">Preparing the Data</h1>\n</div>\n\nIt is now time to prepare the data for machine learning algorithms to train on. There are a few things we need to do.\n\n- **Split into training and testing set**: Using **Stratified Split**, we split the whole dataset into training and testing set such that the testing set is representative of the entire dataset.\n- **Fill in the missing values**: Some features have 0-values which is not logical. So we need to replace them with either *mean* or *median*.\n- **Scaling the feature values**: We apply standard scaling on the feature values, so that the ranges of the features are not too varied.","metadata":{}},{"cell_type":"markdown","source":"# Split into Training and Testing set\n\nSince `Glucose` has the highest positive linear correlation with `Outcome`, we will split the dataset based on the categories of `Glucose`. This is called **Stratified Splitting** with the *strata* being `Glucose` categories. The categories do not exist just yet. We need to create them.\n\nBelow code cell creates a new feature called `Glucose_cat` which divides the `Glucose` feature having range $ [0 - 199] $ into **5 categories**.\nThe categories being:\n\n- $(-1, 40]$\n- $(40, 80]$\n- $(80, 120]$\n- $(120, 160]$\n- $(160, \\infty]$","metadata":{}},{"cell_type":"code","source":"newdf = df","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:56.280337Z","iopub.execute_input":"2022-11-05T11:44:56.281597Z","iopub.status.idle":"2022-11-05T11:44:56.286476Z","shell.execute_reply.started":"2022-11-05T11:44:56.281557Z","shell.execute_reply":"2022-11-05T11:44:56.28509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# segment the dataset into bins\n\nnewdf[\"Glucose_cat\"] = pd.cut(newdf[\"Glucose\"],\n                           bins=[-1, 40, 80, 120, 160, np.inf],\n                          labels=[1, 2, 3, 4, 5])","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:56.287882Z","iopub.execute_input":"2022-11-05T11:44:56.288632Z","iopub.status.idle":"2022-11-05T11:44:56.30342Z","shell.execute_reply.started":"2022-11-05T11:44:56.288597Z","shell.execute_reply":"2022-11-05T11:44:56.302068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newdf[\"Glucose_cat\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:56.30486Z","iopub.execute_input":"2022-11-05T11:44:56.305974Z","iopub.status.idle":"2022-11-05T11:44:56.320752Z","shell.execute_reply.started":"2022-11-05T11:44:56.305927Z","shell.execute_reply":"2022-11-05T11:44:56.3194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig21, ax21 = plt.subplots(figsize=(20, 7))\n\nnewdf[\"Glucose_cat\"].hist(ax=ax21)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:56.322635Z","iopub.execute_input":"2022-11-05T11:44:56.323167Z","iopub.status.idle":"2022-11-05T11:44:56.634344Z","shell.execute_reply.started":"2022-11-05T11:44:56.32313Z","shell.execute_reply":"2022-11-05T11:44:56.632652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using **Scikit-Learn's `Stratified Shuffle Split`** we can split the dataset into Training and Testing set.","metadata":{}},{"cell_type":"code","source":"split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=3301)","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:56.635764Z","iopub.execute_input":"2022-11-05T11:44:56.636151Z","iopub.status.idle":"2022-11-05T11:44:56.641531Z","shell.execute_reply.started":"2022-11-05T11:44:56.636118Z","shell.execute_reply":"2022-11-05T11:44:56.640431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for train_index, test_index in split.split(newdf, newdf[\"Glucose_cat\"]):\n    strat_train_set = newdf.loc[train_index]\n    strat_test_set = newdf.loc[test_index]","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:56.643092Z","iopub.execute_input":"2022-11-05T11:44:56.643407Z","iopub.status.idle":"2022-11-05T11:44:56.659023Z","shell.execute_reply.started":"2022-11-05T11:44:56.643378Z","shell.execute_reply":"2022-11-05T11:44:56.657782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now compare the proportions of various `Glucose` categories between the Testing set and the entire dataset.\nThe output below shows the various proportions.","metadata":{}},{"cell_type":"code","source":"def get_glucose_proportions(ndf):\n    print(ndf[\"Glucose_cat\"].value_counts() / len(ndf))\n\nprint(\"Entire Dataset: \")\nget_glucose_proportions(newdf)\nprint(\"\\n\")\nprint(\"-\"*30)\nprint(\"\\nTesting set: \")\nget_glucose_proportions(strat_test_set)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-11-05T11:44:56.660936Z","iopub.execute_input":"2022-11-05T11:44:56.662183Z","iopub.status.idle":"2022-11-05T11:44:56.674946Z","shell.execute_reply.started":"2022-11-05T11:44:56.662135Z","shell.execute_reply":"2022-11-05T11:44:56.673284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now drop the `Glucose_cat` column to bring the data back to its original form.","metadata":{}},{"cell_type":"code","source":"for set_ in (strat_train_set, strat_test_set):\n    set_.drop(columns=[\"Glucose_cat\"], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:56.677178Z","iopub.execute_input":"2022-11-05T11:44:56.677715Z","iopub.status.idle":"2022-11-05T11:44:56.685809Z","shell.execute_reply.started":"2022-11-05T11:44:56.677662Z","shell.execute_reply":"2022-11-05T11:44:56.684861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fill in the missing values\n\nWe will replace the 0-values of the columns:\n\n- `Glucose`\n- `BloodPressure`\n- `SkinThickness`\n- `Insulin`\n- `BMI`\n\nwith their median values.\n\nHowever, we will store the medians in an array, so that the test set can be replaced by that medians.\n\n> Note: It is essential to split the dataset before performing **Imputation** (replacement of missing values) and **Standardization** to avoid data leaking of the test set into the training set. We want our model to perform good on unseen data.","metadata":{"execution":{"iopub.status.busy":"2022-11-05T10:07:50.375311Z","iopub.execute_input":"2022-11-05T10:07:50.375867Z","iopub.status.idle":"2022-11-05T10:07:50.382492Z","shell.execute_reply.started":"2022-11-05T10:07:50.375827Z","shell.execute_reply":"2022-11-05T10:07:50.381208Z"}}},{"cell_type":"code","source":"# to store medians\nmeds = []\nfeats = [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\"]\n\nfor feat in feats:\n    meds.append(strat_train_set[feat].median())\n    \nprint(\"Medians are: \", meds)","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:56.686857Z","iopub.execute_input":"2022-11-05T11:44:56.687221Z","iopub.status.idle":"2022-11-05T11:44:56.700574Z","shell.execute_reply.started":"2022-11-05T11:44:56.687189Z","shell.execute_reply":"2022-11-05T11:44:56.699189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper function\ndef replace_with_median(ndf, feat, value):\n    ndf[feat] = ndf[feat].replace(0, value)\n    \nfor i, feat in enumerate(feats):\n    replace_with_median(strat_train_set, feat, meds[i])\n    replace_with_median(strat_test_set, feat, meds[i])","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:56.702229Z","iopub.execute_input":"2022-11-05T11:44:56.702634Z","iopub.status.idle":"2022-11-05T11:44:56.714377Z","shell.execute_reply.started":"2022-11-05T11:44:56.702601Z","shell.execute_reply":"2022-11-05T11:44:56.713117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With this, we replaced all the missing values with the median (of that column) in the Training set and used those *learned* medians to replace the missing values in the Testing set.","metadata":{}},{"cell_type":"code","source":"X_train = strat_train_set.drop(columns=\"Outcome\")\ny_train = strat_train_set[\"Outcome\"]\n\nX_test = strat_test_set.drop(columns=\"Outcome\")\ny_test = strat_test_set[\"Outcome\"]","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:56.716123Z","iopub.execute_input":"2022-11-05T11:44:56.716456Z","iopub.status.idle":"2022-11-05T11:44:56.72695Z","shell.execute_reply.started":"2022-11-05T11:44:56.716426Z","shell.execute_reply":"2022-11-05T11:44:56.725602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scaling the features\n\nSince the ranges of different features vary too much, it is necessary to *scale* them so that the Machine Learning models can perform even better. We will use **Scikit-Learn's Standard Scaler** to scale the features.","metadata":{}},{"cell_type":"code","source":"stdscaler = StandardScaler()\nstdscaler.fit(X_train)","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:56.72792Z","iopub.execute_input":"2022-11-05T11:44:56.728265Z","iopub.status.idle":"2022-11-05T11:44:56.742758Z","shell.execute_reply.started":"2022-11-05T11:44:56.728236Z","shell.execute_reply":"2022-11-05T11:44:56.741376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_ = stdscaler.transform(X_train)\nX_test_ = stdscaler.transform(X_test)\n\nprint(\"Scaled training set: \", X_train_)\nprint(\"Scaled testing set: \", X_test_)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-11-05T11:44:56.744159Z","iopub.execute_input":"2022-11-05T11:44:56.744537Z","iopub.status.idle":"2022-11-05T11:44:56.759611Z","shell.execute_reply.started":"2022-11-05T11:44:56.744504Z","shell.execute_reply":"2022-11-05T11:44:56.758211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Done! We now have our Training set `X_train_` (numpy array), Testing set `X_test_` (numpy array) and labels `y_train` and `y_test`, for **supervised learning**.","metadata":{"execution":{"iopub.status.busy":"2022-11-05T10:33:55.691374Z","iopub.execute_input":"2022-11-05T10:33:55.691847Z","iopub.status.idle":"2022-11-05T10:33:55.700762Z","shell.execute_reply.started":"2022-11-05T10:33:55.691812Z","shell.execute_reply":"2022-11-05T10:33:55.699669Z"}}},{"cell_type":"markdown","source":"<div style=\"border-radius: 10px; background-color: #A478B8\">\n    <h1 style=\"color: white; padding: 1rem\">Classification - Supervised Learning</h1>\n</div>\n\nData preparation is done, now its time to run ML algorithms on the preprocessed data. Here, I have written a custom helper function to compare a list of Classifiers, using their **Classification Report**, **Confusion Matrix**, **Precision Recall Curve** and **Reciever Operating Characteristic Curve**.","metadata":{}},{"cell_type":"code","source":"def comp_esti(esti):\n    esti.fit(X_train_, y_train)\n    esti_test_preds = esti.predict(X_test_)\n    \n    print(f\"{esti} Accuracy score: \", accuracy_score(y_test, esti_test_preds))\n    print(f\"\\n{esti} Classification report:\\n\", classification_report(y_test, esti_test_preds, digits=6))\n    \n    # confusion matrix\n    cf_mat = confusion_matrix(y_test, esti_test_preds)\n    fig, ax = plt.subplots(figsize=(10, 8))\n    sns.heatmap(data=cf_mat, annot=True, ax=ax)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:56.761526Z","iopub.execute_input":"2022-11-05T11:44:56.762225Z","iopub.status.idle":"2022-11-05T11:44:56.771021Z","shell.execute_reply.started":"2022-11-05T11:44:56.76219Z","shell.execute_reply":"2022-11-05T11:44:56.769802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we provide a list of classifiers as a parameter to this function...","metadata":{}},{"cell_type":"code","source":"estimators = [\n    RandomForestClassifier(random_state=3301),\n    SVC(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    LogisticRegression(),\n    DecisionTreeClassifier(),\n    KNeighborsClassifier()\n]\n\nfor esti in estimators:\n    comp_esti(esti)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-11-05T11:45:34.08344Z","iopub.execute_input":"2022-11-05T11:45:34.084018Z","iopub.status.idle":"2022-11-05T11:45:36.395977Z","shell.execute_reply.started":"2022-11-05T11:45:34.083955Z","shell.execute_reply":"2022-11-05T11:45:36.394552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Voting classifier\nes1 = RandomForestClassifier(random_state=3301)\nes2 = SVC(probability=True)\nes3 = GradientBoostingClassifier()\n\nesfinal = VotingClassifier(estimators=[\n    (\"rfc\", es1), (\"svc\", es2), (\"grb\", es3)\n], voting=\"soft\")\n\ncomp_esti(esfinal)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-11-05T11:44:58.949664Z","iopub.execute_input":"2022-11-05T11:44:58.950496Z","iopub.status.idle":"2022-11-05T11:44:59.692209Z","shell.execute_reply.started":"2022-11-05T11:44:58.950348Z","shell.execute_reply":"2022-11-05T11:44:59.691072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Note: Which model to choose? That highly depends on what your application is targeted to do. In essence, do you wanna go for high precision or high recall?\n\nHowever, as an example let's fine-tune the **Random Forest Classifier**.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius: 10px; background-color: #A478B8\">\n    <h1 style=\"color: white; padding: 1rem\">Fine-tuning our Model</h1>\n</div>\n\nUsing **Randomized Search CV**, let's try to find better hyperparameters for the **Random Forest Classifier**.","metadata":{}},{"cell_type":"code","source":"# Parameters of random forest classifier\nn_estimators = np.linspace(50, 300, int((300 - 50) / 20), dtype=int)\nmax_depth = [1, 5, 10, 50, 100, 200, 300]\nmin_samples_split = [2, 4, 6]\nmax_features = [\"sqrt\", \"log2\"]\nbootstrap = [True, False]\n\ndistributions = {\n    \"n_estimators\": n_estimators,\n    \"max_depth\": max_depth,\n    \"min_samples_split\": min_samples_split,\n    \"max_features\": max_features,\n    \"bootstrap\": bootstrap\n}","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:44:59.694079Z","iopub.execute_input":"2022-11-05T11:44:59.694444Z","iopub.status.idle":"2022-11-05T11:44:59.702626Z","shell.execute_reply.started":"2022-11-05T11:44:59.694412Z","shell.execute_reply":"2022-11-05T11:44:59.701374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Randomised search cv\nfrom sklearn.model_selection import RandomizedSearchCV\n\nrfc = RandomForestClassifier(random_state=3301)\nrandom_search_cv = RandomizedSearchCV(\n    rfc,\n    param_distributions=distributions,\n    n_iter=30,\n    cv=5,\n    n_jobs=4\n)\n\nsearch = random_search_cv.fit(X_train_, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:45:46.05702Z","iopub.execute_input":"2022-11-05T11:45:46.057609Z","iopub.status.idle":"2022-11-05T11:46:04.977634Z","shell.execute_reply.started":"2022-11-05T11:45:46.05757Z","shell.execute_reply":"2022-11-05T11:46:04.97596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The results of this Randomized Search are stored in a dictionary named `cv_results_`. Let us print these results just to get an idea of what parameters were tested by our Randomized Search.","metadata":{}},{"cell_type":"code","source":"cvres = search.cv_results_\n\nfor score, params, rank in zip(cvres[\"mean_test_score\"], cvres[\"params\"], cvres[\"rank_test_score\"]):\n    print(score, params, rank)\n    ","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-11-05T11:47:42.836538Z","iopub.execute_input":"2022-11-05T11:47:42.837287Z","iopub.status.idle":"2022-11-05T11:47:42.850144Z","shell.execute_reply.started":"2022-11-05T11:47:42.837248Z","shell.execute_reply":"2022-11-05T11:47:42.849183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also, the best estimator out of these tested ones is stored in a variable called `best_estimator_`. We can use this estimator as our fine-tuned model.","metadata":{}},{"cell_type":"code","source":"rfc_finetuned = search.best_estimator_\nrfc_finetuned.fit(X_train_, y_train)\n\nbest_preds = rfc_finetuned.predict(X_test_)\n\nfig, ax = plt.subplots(1, 2, figsize=(20, 10))\nPrecisionRecallDisplay.from_predictions(y_test, best_preds, ax=ax[0])\nRocCurveDisplay.from_predictions(y_test, best_preds, ax=ax[1])\n\nprint(classification_report(y_test, best_preds, digits=5))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-05T11:47:49.391301Z","iopub.execute_input":"2022-11-05T11:47:49.391845Z","iopub.status.idle":"2022-11-05T11:47:50.397205Z","shell.execute_reply.started":"2022-11-05T11:47:49.3918Z","shell.execute_reply":"2022-11-05T11:47:50.395653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, by fine-tuning the **Random Forest Classifier**:\n\n- We increased the average precision from **0.68** to **0.70**.\n- We increased the average recall from **0.67** to **0.69**.\n- We increased the accuracy from **70%** to **72%**.","metadata":{}},{"cell_type":"markdown","source":"**<span style=\"font-size: 14px; color: darkblue\">If this notebook helped you a slightest bit, do consider upvoting it and leaving a comment below! Feel free to extend this notebook with more knowledge. Thank you! ❤️</span>**","metadata":{}}]}